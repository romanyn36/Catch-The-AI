


<p align="center" id="top"> 
<img src="/images/screens/1 home.jpg" alt="x-ray" width="450" height="300" />

</p>

<h1 align="center">Catch The AI</h1>

<p align="center">





<!-- TABLE OF CONTENTS -->
#### Table of Contents
  <ol>
    <li><a href="#dart-about">About</a></li>
    <li><a href="#sparkles-features">Features</a></li>
    <li><a href="#rocket-technologies">Technologies</a></li>
    <li><a href="#white_check_mark-requirements">Requirements</a></li>
    <li><a href="#checkered_flag-starting">Starting</a></li>
    <li><a href="#busts_in_silhouette-collaborators">collaborators</a></li>
    <li><a href="#email-contact">Contact</a></li>
    <li><a href="#memo-license">License</a></li>
    <li><a href="https://github.com/romanyn36" target="_blank">Author</a></li>
  </ol>


<br>

# :dart: About ##
Catch The AI is our graduation project, it is an
Intelligent Sytem to detect AI-generated content with our advanced models. Our deep learning technology distinguishes between AI and human-authored media in images and text, 
we deployed our model in a web application to make it easy to use for everyone. you can visit the website [Catch The AI](https://catch-the-ai.tech/)

##### full project in details in the [documentation](/Documentaion/Documentation%20Library/Final%20versions%20of%20DOC/Project_Documentation.pdf)

# project Overview

## :sparkles: Features ##

:heavy_check_mark: Detect AI-generated content in images, text, and audio.
:heavy_check_mark: User-friendly web application.
:heavy_check_mark: Full User authentication and authorization system.
:heavy_check_mark: detecting history for each user.
:heavy_check_mark: Admin panel to manage users and their data.
:heavy_check_mark: Supscription system to get more features (coming soon).

 <!-- Data Sources and Datasets used in the project -->
Here's a concise summary for your README:

---

## üöÄ Data Sources and Datasets

This project utilizes a range of datasets to train and test the AI detection models. The datasets are categorized by the type of media they contain, including images, audio, and text. The datasets are sourced from various repositories and research projects, providing a diverse and comprehensive collection of AI-generated content for model training and evaluation.


### Audio Data
- **Fake-or-Real Dataset (FoR):** Baseline detection with genuine and fake audio samples [32].
- **Scenefake Dataset:** Diverse deepfake audio clips from various techniques [33].
- **In the Wild Dataset:** Real and fake audio from diverse internet sources [34].
- **ASVspoof 2019 Dataset:** Authentic and spoofed audio for ASV challenges [35].
- **ASVspoof 2021 Dataset:** Updated spoofed audio reflecting advancements in deepfake technology [36].

### Image Data
- **140k Real and Fake Faces:** 70,000 real faces from Flickr and 70,000 StyleGAN-generated faces, resized to 256x256 [37].
- **CelebA-HQ (256x256):** 30,000 high-quality celebrity faces for model training [38].
- **Synthetic Faces High Quality (SFHQ) Part 2:** 91,361 curated faces at 1024x1024, enhanced by StyleGAN2 [39].
- **Face Dataset Using Stable Diffusion v1.4:** Real and fake faces, resized to 256x256, using Stable Diffusion models [40].
- **Stable Diffusion Face Dataset:** AI-generated faces at 512x512, 768x768, and 1024x1024 resolutions using Stable Diffusion checkpoints [41].
- **Synthetic Faces High Quality (SFHQ) Part 3:** 118,358 faces at 1024x1024, generated by StyleGAN2 with advanced techniques [42].
- **Synthetic Human Faces for 3D Reconstruction:** High-quality 512x512 faces generated using the EG3D model for 3D reconstruction [43].

### Text Data
- **LLM Generated Essays for the Detect AI Comp:** 700 essays, including 500 generated with GPT-3.5-turbo and 200 with GPT-4.
- **DAIGT Data - Llama 70b and Falcon 180b [44]:**
  - **Llama Falcon v3:** 7,000 LLM-generated essays.
  - **Llama 70b v2:** 1,172 LLM-generated essays.
  - **Llama 70b v1:** 1,172 LLM-generated essays.
  - **Falcon 180b v1:** 1,055 LLM-generated essays.
- **Persuade Corpus 2.0:** Over 25,000 argumentative essays by U.S. students in grades 6-12.
- **DAIGT External Dataset:** 2,421 student-generated texts and 2,421 AI-generated texts for balanced training data.



---

## üìä Models selection and Initial results 

### Text Models
- **BERT:** Achieved 90% accuracy but showed signs of overfitting on smaller datasets.
- **RoBERTa:** Outperformed BERT with 99% accuracy and demonstrated better generalization.
- **DeBERTa:** Achieved the highest accuracy of 99%, showing superior handling of complex text patterns.

### Audio Models
- **Wav2Vec2:** Excelled with a word error rate of 7% and robust anomaly detection.
- **Mel-spectrogram + CNN:** Delivered reasonable accuracy but was less effective in detecting subtle anomalies compared to Wav2Vec2.
- **ResNet-based Model:** Provided good results but was more computationally intensive.

### Image Models
- **EfficientNet:** Balanced accuracy and computational efficiency, achieving 99% accuracy.
- **ResNet:** Reached 99% accuracy but required more computational resources.
- **Xception:** Offered detailed feature extraction but was less efficient compared to EfficientNet. 
### Evaluation Criteria


## üèÜ Final Model

### Text
- **Ensemble of RoBERTa and DeBERTa:** Combines the outputs of both models and integrates them through a final linear layer to enhance overall classification performance.
  - **Architecture:**
    - **RoBERTa Output:** Captures robust language patterns.
    - **DeBERTa Output:** Provides nuanced language understanding.
    - **Final Linear Layer:** Integrates the concatenated outputs to improve classification.<br>
    ## For more details, about text models please check the [DAIGT-Catch-the-AI](https://github.com/zeyadusf/DAIGT-Catch-the-AI)

### Audio
- **Wav2Vec2:** Selected for its state-of-the-art performance in audio anomaly detection.

### Image
- **EfficientNet:** Chosen for its efficiency and high accuracy in distinguishing real from AI-generated images.

The ensemble model for text was validated on additional test datasets, confirming its robustness and ability to generalize across various scenarios. This ensemble approach demonstrated significant improvements over individual models, providing a more comprehensive understanding and classification of text inputs.




<!-- 

## :rocket: Technologies ##
<p align='center'>
<img src=https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original-wordmark.svg width="45" height="45" />


<img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/tensorflow/tensorflow-original.svg" alt="TensorFlow" width="45" height="45" />

<img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/opencv/opencv-original.svg" alt="OpenCV" width="45" height="45" />

<img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/vscode/vscode-original.svg" alt="vscode" width="45" height="45"/>

<img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/git/git-original.svg" alt="git" width="45" height="45"/>

<img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/jupyter/jupyter-original-wordmark.svg" alt="Jupyter" width="45" height="45" />
 
</p>

The following tools were used in this project: -->

## :checkered_flag: Starting ##
first clone the repo  
```bash
# Clone this project
$ git clone https://github.com/romanyn36/Catch-The-AI.git
 
#  go to the project folder
$ cd Catch-The-AI

# create a virtual environment
$ python -m venv myenv

# activate the virtual environment
$ source myenv/bin/activate # for linux
$ myenv\Scripts\activate # for windows

# install the requirements
$ pip install -r requirements.txt

# great! now you can run the project 
# now to run the Django server
# go to the Backend Django folder
$ cd Backend

# run the server
$ python manage.py runserver


# now to run the React app
# go to the Frontend React folder
$ cd ../Frontend 

cd catch-the-ai

# install the dependencies
$ npm install

# run the app
$ npm start

the app will run on [http://localhost:3000/](http://localhost:3000/)

```



## :busts_in_silhouette: collaborators ##
We did not just work as a team, but we were a family. These people are truly skilled and creative. Follow them and wait for their wonderful projects, from which they learn a lot and benefit a lot of people.‚ù§Ô∏è <br>
- Romani Nasrat (Team Leader): 
<p align="center">work on Image detection model, and responsible for the backend development connected with the frontend and the database, and also responsible for the deployment of the project.</p>

- Ahmed Mohamed Ali: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[![](https://img.shields.io/badge/-@AhmedAboElkassem-181717?style=flat&logo=github&logoColor=white)](https://github.com/AhmedAboElkassem) [![](https://img.shields.io/badge/-Ahmed_Ali-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/mwlite/in/ahmed-ali-54292924b)

- Reham Mustafa&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[![](https://img.shields.io/badge/-Reham_Mustafa-181717?style=flat&logo=github&logoColor=white)](https://github.com/Reham-Mustafa) [![](https://img.shields.io/badge/-Reham_Mustafa-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/reham-mustafa-9a321b263/)

- Sara Reda Moatamed  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[![](https://img.shields.io/badge/-Sara_reda-181717?style=flat&logo=github&logoColor=white)](https://github.com/SaraReda8) [![](https://img.shields.io/badge/-Reham_Mustafa-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/sara-reda-5ab46b317/)


- Ziad El-Sayed Abdel-Azim&nbsp;&nbsp;&nbsp;&nbsp;[![](https://img.shields.io/badge/-@zeyadusf-181717?style=flat&logo=github&logoColor=white)](https://github.com/zeyadusf) [![](https://img.shields.io/badge/-Zeyad_Usf-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/zeyad-usf-360154214/)

 - Rawan Abdel-Aziz Ahmed&nbsp;&nbsp;&nbsp;&nbsp;[![](https://img.shields.io/badge/-@rawanazizsaad-181717?style=flat&logo=github&logoColor=white)](https://github.com/rawanazizsaad) [![](https://img.shields.io/badge/-Email-D14836?style=flat&logo=mail.ru&logoColor=white)](mailto:rawansaad222222@gmail.com)

### :email: Contact ##

<p align="center">
 <a href="https://www.facebook.com/romanyn3/" target="_blank">
  <img src="https://img.shields.io/badge/-Romani-1877F2?style=flat&logo=facebook&logoColor=white" alt="Facebook" />
</a>

<a href="https://twitter.com/romanyn36" target="_blank">
  <img src="https://img.shields.io/badge/-@romanyn36-1DA1F2?style=flat&logo=twitter&logoColor=white" alt="Twitter" />
</a>


<a href="https://github.com/romanyn36" target="_blank">
  <img src="https://img.shields.io/badge/-@romanyn36-181717?style=flat&logo=github&logoColor=white" alt="GitHub" />
</a>


<a href="https://stackoverflow.com/users/17348975/romani" target="_blank">
  <img src="https://img.shields.io/badge/-Stack%20Overflow-FE7A16?style=flat&logo=stackoverflow&logoColor=white" alt="Stack Overflow" />
</a>

<a href="mailto:romanyyy36dr99@gmail.com" target="_blank">
  <img src="https://img.shields.io/badge/-Email-D14836?style=flat&logo=mail.ru&logoColor=white" alt="Email" />
</a>

<a href="https://www.linkedin.com/in/romanyn36/" target="_blank">
    <img src="https://img.shields.io/badge/Connect-Romani-blue.svg?style=flat&logo=linkedin" alt="linkedin"/>
</a>




<a href="https://www.kaggle.com/romanyn36" target="_blank" style="display: inline-block;">
    <img src="https://img.shields.io/badge/Kaggle-Romani-blue.svg?style=flat-square&logo=kaggle" />
</a>
<a href="https://www.buymeacoffee.com/romanyn36" target="_blank" style="display: inline-block;">
    <img src="https://img.shields.io/badge/Donate-Buy%20Me%20A%20Coffee-orange.svg?style=flat-square&logo=buymeacoffee" align="center"/>
  </a> 
  

</p>

&#xa0;

<a href="#top">Back to top</a>


